{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text autorship identification\n",
    "\n",
    "For this prove, we will test ideas to feature extraction and data pre-processing that will be taken from two papers found in arxiv:\n",
    "\n",
    "[1] [TEXT CLASSIFICATION FOR AUTHORSHIP\n",
    "ATTRIBUTION ANALYSIS](https://arxiv.org/pdf/1310.4909.pdf)\n",
    "\n",
    "[2] [A Machine Learning Framework for Authorship Identification From Texts](https://arxiv.org/pdf/1912.10204.pdf)\n",
    "\n",
    "In our tests, we will check how the accuracy of our final model behaves when presented to these ideas.\n",
    "\n",
    "This notebook is part of a practical prove provided from NUVEO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "For the purpose of this task, the following libraries will be helping us to present the ideas through this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re \n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_data = pd.read_csv('./TrainingSet/text-authorship-training.csv')\n",
    "author_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = 'EAP','MWS','HPL'\n",
    "author_data['author'].value_counts()\n",
    "\n",
    "sizes = list(author_data['author'].value_counts())\n",
    "\n",
    "explode=(0.05, 0, 0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(sizes, explode=explode, labels=labels, shadow=True,autopct='%1.1f%%', startangle=140)\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the authors distribution looks be very balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "As mentioned at the beginning, the main ideas for feature extraction were taking from two papers.From the first one, we will take features that involves punctuation and features related directly with the text and letters. From the second one, we will count the occurence of some certains words. To sum up, here it is the features that we will implement:\n",
    "\n",
    "**Punctuation and Phraseology:**\n",
    "\n",
    "- Number of periods\n",
    "- Number of commas\n",
    "- number of question marks\n",
    "- Number of colons\n",
    "- Number of semi-colons\n",
    "- Number of blanks \n",
    "- Number of exclamation marks\n",
    "- Number of dashes\n",
    "- Number of underscores\n",
    "- Number of brackets\n",
    "- Number of words\n",
    "- Number of sentences\n",
    "- Number of characters\n",
    "\n",
    "**Lexical:**\n",
    "- Number of and\n",
    "- Number of but\n",
    "- Number of however\n",
    "- Number of if\n",
    "- Number of that\n",
    "- Number of more\n",
    "- Number of might\n",
    "- Number of this\n",
    "- Number of very\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_punc_phras(author_data):\n",
    "    \"\"\" Extract features from punctuation and phraseology\n",
    "    \n",
    "    Args:\n",
    "        author_data: the dataframe with the text information\n",
    "    Return:\n",
    "        author_data: the dataframe that now contains the new features\n",
    "    \"\"\"\n",
    "    \n",
    "    author_data['num_periods'] = author_data['text'].apply(lambda x: x.count('.'))\n",
    "    author_data['num_commas'] = author_data['text'].apply(lambda x: x.count(','))\n",
    "    author_data['num_questions'] = author_data['text'].apply(lambda x: x.count('?'))\n",
    "    author_data['num_colons'] = author_data['text'].apply(lambda x: x.count(':'))\n",
    "    author_data['num_semi-colons'] = author_data['text'].apply(lambda x: x.count(';'))\n",
    "    author_data['num_blanks'] = author_data['text'].apply(lambda x: x.count(' '))\n",
    "    author_data['num_exclamation'] = author_data['text'].apply(lambda x: x.count('!'))\n",
    "    author_data['num_dashes'] = author_data['text'].apply(lambda x: x.count('-'))\n",
    "    author_data['num_underscores'] = author_data['text'].apply(lambda x: x.count('_'))\n",
    "    author_data['num_brackets'] = author_data['text'].apply(lambda x: x.count('[') * 2)\n",
    "    \n",
    "    author_data['num_words'] = author_data['text'].apply(lambda x: len(x.split(' ')))\n",
    "    author_data['num_sentences'] = author_data['text'].apply(lambda x: len(sent_tokenize(x)))\n",
    "    author_data['num_characters'] = author_data['text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "    return author_data\n",
    "\n",
    "def feature_extraction_lexical(author_data):\n",
    "    \"\"\" Extract features from lexical\n",
    "    Args:\n",
    "        author_data: the dataframe with the text information\n",
    "    Return:\n",
    "        author_data: the dataframe that now contains the new features\n",
    "    \"\"\"\n",
    "    lexical_terms = [\"and\", \"but\", \"however\",\"if\",\"that\",\"more\",\"might\",\"this\",\"very\"]\n",
    "    \n",
    "    for lexical in lexical_terms:\n",
    "        author_data[f'num_of_{lexical}'] = author_data['text'].apply(lambda x: x.count(lexical))\n",
    "    \n",
    "    return author_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all feature extaction, our data now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_data = feature_extraction_punc_phras(author_data)\n",
    "author_data = feature_extraction_lexical(author_data)\n",
    "author_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check how one of those metrics behaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "Following the ideas from both papers, to process our data we will use 3 methods:\n",
    "\n",
    "**1. Tokenization:** Tokenization is the method of splitting a stream of text into meaningful element. For us, these meaningful elements will be taken following the stopwords/punctuation idea, in other words, only not stopwords/punctuation will continue in our stream of text. We also will put all words in lower case.\n",
    "\n",
    "**2. Stemming:** Stemming is the process of reducing the inflected words to their root or base form known as stem.\n",
    "The stem may not be same as the morphological root of that word. [1] uses WordNet for stemming. This stemmer adds functionality to the simple pattern-based stemmer SimpleStemmer by checking to see if possible stems are actually present in Wordnet. \n",
    "\n",
    "**3. Top K Words:** Mapping all words present in the dataset, build a corpus which, this corpus, contains the frequency of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_normalization(message):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    stopwords.words('english')\n",
    "    word_tokens = word_tokenize(message) \n",
    "    filtered_stop = [word for word in word_tokens if not word in stopwords.words('english')]\n",
    "    filtered_punctuation = [word for word in filtered_stop if not word in string.punctuation]\n",
    "\n",
    "    return \" \".join([word.lower() for word in filtered_punctuation])\n",
    "    \n",
    "def stemming_normalization(message):\n",
    "    \"\"\"Stemming the given message using WordNetLemmatizer()\n",
    "    Args:\n",
    "        message: sms string\n",
    "    Returns:\n",
    "        return a stemmed sentence\n",
    "    \"\"\"\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    splited_message = message.split()\n",
    "    \n",
    "    return \" \".join([wnl.lemmatize(word) for word in splited_message])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_data['tokenized_text'] = author_data['text'].apply(lambda x: tokenize_normalization(x))\n",
    "author_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying stemming normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_data['stemmed_text'] = author_data['tokenized_text'].apply(lambda x: stemming_normalization(x))\n",
    "author_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(author_data.stemmed_text)\n",
    "count_vect_df = pd.DataFrame(X.toarray())\n",
    "count_vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"At the final, vectorizer corpus has a total of {} words\".format(len(count_vect_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat([author_data, count_vect_df], axis=1, sort=False)\n",
    "final_data.drop(['author','text','id','tokenized_text','stemmed_text'], axis=1, inplace=True)\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_train, txt_test, label_train, label_test = train_test_split(final_data, author_data['author'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Note</span>\n",
    "\n",
    "The first idea here was test the data using SVM classifier, just like [1] did. But it was taking a long time to train and I stoped the process to try something else.\n",
    "\n",
    "My second idea was try to use grid search just like I did in the [first](https://github.com/joaolcaas/cv-test/blob/JoaoFelipe/SMSSpamDetection/SMSSpamDetection.ipynb) prove. But some erros happened (i.e. memory error) and I was not able to run this type of strategy due the (1) big data and (2) small memory at the train setup computer.\n",
    "\n",
    "My third option was decrease the number of parameters for search, which you can check below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators'      : [50, 150,340],\n",
    "    'max_depth'         : [8, 30, None],\n",
    "}\n",
    "gs = GridSearchCV(model, parameters)\n",
    "gs_fit = gs.fit(txt_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_fit.best_params_)\n",
    "print(gs_fit.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_prediction = clf.predict(txt_test)\n",
    "print(\"Accuracy: {}\".format( round((clf_prediction==label_test).sum() / len(clf_prediction),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF comparisson\n",
    "\n",
    "None of both papers does use tf-idf approach. In order to compare both approaches, we will train the same data, changing CountVectorizer() per TfidfVectorizer() and see if the accuracie changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing this due the memory error\n",
    "del final_data\n",
    "del gs\n",
    "del gs_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_vec = vectorizer.fit_transform(author_data.stemmed_text)\n",
    "tfidf_vect_df = pd.DataFrame(tf_idf_vec.toarray())\n",
    "tfidf_vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"At the final, vectorizer tf-idf has a total of {} words\".format(len(tfidf_vect_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat([author_data, count_vect_df], axis=1, sort=False)\n",
    "final_data.drop(['author','text','id'], axis=1, inplace=True)\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_train, txt_test, label_train, label_test = train_test_split(final_data, author_data['author'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators'      : [50, 150,340],\n",
    "    'max_depth'         : [8, 30, None],\n",
    "}\n",
    "gs = GridSearchCV(model, parameters)\n",
    "gs_fit = gs.fit(txt_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs_fit.best_params_)\n",
    "print(gs_fit.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_prediction = clf.predict(txt_test)\n",
    "print(\"Accuracy: {}\".format( round((clf_prediction==label_test).sum() / len(clf_prediction),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('./TestSet/text-authorship-test.csv')\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = feature_extraction_punc_phras(test_set)\n",
    "test_set = feature_extraction_lexical(test_set)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['tokenized_text'] = test_set['text'].apply(lambda x: tokenize_normalization(x))\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['stemmed_text'] = test_set['tokenized_text'].apply(lambda x: stemming_normalization(x))\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_set.stemmed_text)\n",
    "vector_test = pd.DataFrame(X_test.toarray())\n",
    "vector_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.drop(['stemmed_text'],axis=1, inplace=True)\n",
    "final_data_test = pd.concat([test_set, vector_test], axis=1, sort=False)\n",
    "final_data_test.drop(['text','id','tokenized_text'], axis=1, inplace=True)\n",
    "final_data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_set = {\"author\":[],\"text\":test_set.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(final_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_set[\"author\"] = predictions\n",
    "final_ans = pd.DataFrame(final_set)\n",
    "final_ans.to_csv('final_ans.csv')\n",
    "final_ans.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
